############################################################################################
# Haupt-Konfiguration der Suchmaschine                                                     #
############################################################################################

local: &default
  # Verbindung zur Datenbank, "druby://0.0.0.0:2051" bedeutet lokal.
  database_connection: "druby://0.0.0.0:2051"
  # Parameter der Datenbank
  database:
    # Grösse eines BetterQueue-Stapels in Anzahl Zeilen.
    batch_size: 100_000
    # Blockgrössen der verschiedenen LevelDB Datenbanken in KB.
    block_size:
      document: 128
      metadata: 4
      cache: 4
      search_cache: 4
      postings_block: 32
      postings_metadata: 8
    # Whitelist der erlaubten IPs, welche eine Verbindung zur Datenbank herstellen dürfen.
    client_whitelist:
      - 127.0.0.1
      # Weitere IPs hier auflisten.
  # Parameter des Crawlers
  crawler:
    # User-Agent der bei HTTP-Anfragen mitgesendet werden soll.
    agent: "breaksearch.crawler/1.0"
    # Anzahl Threads, die für das Crawling verwendet werden sollen.
    threads: 100
    # Timeout von HTTP-Anfragen in Sekunden.
    # Sollte eine Anfrage länger dauern, wird sie als Fehler abgebgrochen.
    timeout: 10
    # Maximale Grösse einer HTTP-Antwort in Bytes.
    # Überschüssiger Inhalt wird abgetrennt und nicht empfangen.
    maxsize: 500_000
  # Konfiguration des robots.txt-Lesers
  robotstxt:
    cache:
      # Cache aktiviert?
      enabled: YES
      # Maximales Alter von Cache-Einträgen in Sekunden.
      lifetime: 86400
  # Konfiguration des Indexierers.
  indexer:
    # Anzahl Threads, die für die Indexierung verwendet werden sollen.
    threads: 20
  # Konfiguration der Dateipfade für die Speicherung der Daten.
  # Ordner müssen mit einem Slash beendet werden.
  # Die Pfade sollten alle absolute Pfade sein, eventuell funktioniert das
  # verwenden der ~ für das Homeverzeichnis oder relative Verzeichnise,
  # allerdings wurde der Code diesbezüglich nicht optimiert und es müsste mit
  # teilweise höheren Performanceinbussen gerechnet werden.
  paths:
    # Pfad zum Ordner in dem die Cache-Einträge gespeichert werden.
    cache: "/ssd/suchmaschine/cache/"
    # Pfad zum Ordner in dem die Dokumente gespeichert werden.
    document: "/hdd/suchmaschine/db/document/"
    # Pfad zum Ordner in dem die Download-Warteschlange gespeichert wird.
    download_queue: "/hdd/suchmaschine/db/download_q/"
    # Pfad zur Index-Datei.
    index: "/ssd/suchmaschine/index"
    # Pfad zum Ordner in dem die temporären Index-Dateien gespeichert werden.
    index_tmp: "/hdd/suchmaschine/tmp/index/"
    # Pfad zum Ordner in dem die Index-Warteschlange gespeichert wird.
    index_queue: "/hdd/suchmaschine/db/index_q/"
    # Pfad zum Ordner in dem die Metadaten gespeichert werden.
    metadata: "/ssd/suchmaschine/metadata/"
    # Pfad zum Ordner in dem die SearchCache-Einträge gespeichert werden.
    search_cache: "/ssd/suchmaschine/search_cache/"

# Unter Umständen werden mehrere Profile benötigt, hierzu kann man diese hier
# definieren. Die Standardwerte werden übernommen, allerdings heisst das überschreiben
# der Werte für einen Schlüssel (in der obersten Hierarchie), dass keine Standardwerte
# für diesen Eintrag übernommen werden.

# my-cool-laptop:
#   <<: *default
#   database_connection: "druby://192.168.10.50:2051"
#   paths:
#     cache: "/pfad/zur/db/cache/"
#     search_cache: "/pfad/zur/db/search_cache/"
#     document: "/pfad/zur/db/document/"
#     download_queue: "/pfad/zur/db/download_q/"
#     index: "/pfad/zur/db/index"
#     index_tmp: "/pfad/zur/tmp/index/"
#     index_queue: "/pfad/zur/db/index_q/"
#     metadata: "/pfad/zur/db/metadata/"
